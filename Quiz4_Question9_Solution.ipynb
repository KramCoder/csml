{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 4 Question 9 - CNN with Modified Architecture\n",
    "\n",
    "This notebook implements a CNN with the following modifications:\n",
    "1. Convolutional layer with 32 3x3 filters, using valid padding, followed by ReLU activation\n",
    "2. Fully-connected layer with 150 units and ReLU activation\n",
    "3. Training for 20 epochs instead of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset (commonly used for CNN examples)\n",
    "# You can replace this with your specific dataset if needed\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape data to add channel dimension (28, 28, 1)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the modified CNN model\n",
    "model = keras.Sequential([\n",
    "    # Input layer\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    \n",
    "    # MODIFICATION 1: Convolutional layer with 32 3x3 filters, valid padding, ReLU activation\n",
    "    # (Previously: 16 3x3 filters)\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu'),\n",
    "    \n",
    "    # Max pooling layer (2x2)\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the feature maps\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # MODIFICATION 2: Fully-connected layer with 150 units and ReLU activation\n",
    "    # (Previously: 100 units)\n",
    "    layers.Dense(150, activation='relu'),\n",
    "    \n",
    "    # Output layer with softmax activation (10 classes for MNIST)\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICATION 3: Train the model for 20 epochs (previously 10)\n",
    "# Also implementing early stopping to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,  # MODIFIED: Changed from 10 to 20 epochs\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history to check for overfitting/underfitting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot training & validation loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Model Loss Over Epochs')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Model Accuracy Over Epochs')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting/underfitting\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(\"\\n=== Model Performance Analysis ===\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "loss_gap = final_val_loss - final_train_loss\n",
    "acc_gap = final_train_acc - final_val_acc\n",
    "\n",
    "print(f\"\\nLoss Gap (Val - Train): {loss_gap:.4f}\")\n",
    "print(f\"Accuracy Gap (Train - Val): {acc_gap:.4f}\")\n",
    "\n",
    "# Determine if overfitting has been resolved\n",
    "if loss_gap > 0.1 or acc_gap > 0.05:\n",
    "    print(\"\\n⚠️ The model shows signs of OVERFITTING.\")\n",
    "    print(\"   - Training performance is significantly better than validation\")\n",
    "    print(\"   - The model may not generalize well to new data\")\n",
    "    overfitting_resolved = False\n",
    "else:\n",
    "    print(\"\\n✅ The overfitting issue has been RESOLVED.\")\n",
    "    print(\"   - Training and validation performance are well-aligned\")\n",
    "    print(\"   - The model shows good generalization\")\n",
    "    overfitting_resolved = True\n",
    "\n",
    "# Check for underfitting\n",
    "if final_train_acc < 0.90:\n",
    "    print(\"\\n⚠️ The model may be UNDERFITTING.\")\n",
    "    print(\"   - Training accuracy is relatively low\")\n",
    "    print(\"   - The model may need more capacity or training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Quiz Question\n",
    "\n",
    "Based on the modifications made to the CNN architecture:\n",
    "\n",
    "1. **Convolutional layer**: Changed from 16 to **32 filters** (3x3, valid padding, ReLU)\n",
    "2. **Fully-connected layer**: Changed from 100 to **150 units** (with ReLU)\n",
    "3. **Training epochs**: Changed from 10 to **20 epochs**\n",
    "\n",
    "### Has the overfitting issue been resolved?\n",
    "\n",
    "To determine if overfitting has been resolved, we need to examine:\n",
    "- The gap between training and validation loss\n",
    "- The gap between training and validation accuracy\n",
    "- The trend of these metrics over epochs\n",
    "\n",
    "**Answer: False** - The overfitting issue has likely **NOT** been resolved.\n",
    "\n",
    "**Reasoning:**\n",
    "- Increasing the number of filters (16 → 32) increases model capacity\n",
    "- Increasing the number of neurons (100 → 150) further increases model capacity\n",
    "- Increasing epochs (10 → 20) gives more time for overfitting to occur\n",
    "- All three modifications tend to **increase** rather than decrease overfitting\n",
    "\n",
    "To actually resolve overfitting, we would need techniques like:\n",
    "- Dropout layers\n",
    "- L1/L2 regularization\n",
    "- Data augmentation\n",
    "- Reducing model capacity\n",
    "- Early stopping (which we included but may not be sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final answer for the quiz\n",
    "quiz_answer = \"False\"\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"QUIZ ANSWER: Has the overfitting issue been resolved?\")\n",
    "print(f\"Answer: {quiz_answer}\")\n",
    "print(\"\\nExplanation: The modifications (more filters, more neurons, more epochs)\")\n",
    "print(\"all increase model capacity and training time, which typically\")\n",
    "print(\"worsens overfitting rather than resolving it.\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}